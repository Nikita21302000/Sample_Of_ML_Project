{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1Dgwg_84yP9jx8M3PJ4pcrtDE1h3nsrE5","timestamp":1740242684377},{"file_id":"https://gist.github.com/Nikita21302000/883198496da07f79224827b37eb9ff8a#file-sample-ml-submission-template-ipynb","timestamp":1739988614758}],"collapsed_sections":["vncDsAP0Gaoa","FJNUwmbgGyua","w6K7xa23Elo4","yQaldy8SH6Dl","mDgbUHAGgjLW","O_i_v8NEhb9l","HhfV-JJviCcP","Y3lxredqlCYt","3RnN4peoiCZX","x71ZqKXriCWQ","7hBIi_osiCS2","JlHwYmJAmNHm","35m5QtbWiB9F","PoPl-ycgm1ru","H0kj-8xxnORC","nA9Y7ga8ng1Z","PBTbrJXOngz2","u3PMJOP6ngxN","dauF4eBmngu3","bKJF3rekwFvQ","MSa1f5Uengrz","GF8Ens_Soomf","0wOQAZs5pc--","K5QZ13OEpz2H","lQ7QKXXCp7Bj","448CDAPjqfQr","KSlN3yHqYklG","t6dVpIINYklI","ijmpgYnKYklI","-JiQyfWJYklI","EM7whBJCYoAo","fge-S5ZAYoAp","85gYPyotYoAp","RoGjAbkUYoAp","4Of9eVA-YrdM","iky9q4vBYrdO","F6T5p64dYrdO","y-Ehk30pYrdP","bamQiAODYuh1","QHF8YVU7Yuh3","GwzvFGzlYuh3","qYpmQ266Yuh3","OH-pJp9IphqM","bbFf2-_FphqN","_ouA3fa0phqN","Seke61FWphqN","PIIx-8_IphqN","t27r6nlMphqO","r2jJGEOYphqO","b0JNsNcRphqO","BZR9WyysphqO","jj7wYXLtphqO","eZrbJ2SmphqO","rFu4xreNphqO","YJ55k-q6phqO","gCFgpxoyphqP","OVtJsKN_phqQ","lssrdh5qphqQ","U2RJ9gkRphqQ","1M8mcRywphqQ","tgIPom80phqQ","JMzcOPDDphqR","x-EpHcCOp1ci","X_VqEhTip1ck","8zGJKyg5p1ck","PVzmfK_Ep1ck","n3dbpmDWp1ck","ylSl6qgtp1ck","ZWILFDl5p1ck","M7G43BXep1ck","Ag9LCva-p1cl","E6MkPsBcp1cl","2cELzS2fp1cl","3MPXvC8up1cl","NC_X3p0fY2L0","UV0SzAkaZNRQ","YPEH6qLeZNRQ","q29F0dvdveiT","EXh0U9oCveiU","22aHeOlLveiV","g-ATYxFrGrvw","Yfr_Vlr8HBkt","8yEUt7NnHlrM","tEA2Xm5dHt1r","I79__PHVH19G","Ou-I18pAyIpj","fF3858GYyt-u","4_0_7-oCpUZd","hwyV_J3ipUZe","3yB-zSqbpUZe","dEUvejAfpUZe","Fd15vwWVpUZf","bn_IUdTipZyH","49K5P_iCpZyH","Nff-vKELpZyI","kLW572S8pZyI","dWbDXHzopZyI","yLjJCtPM0KBk","xiyOF9F70UgQ","7wuGOrhz0itI","id1riN9m0vUs","578E2V7j08f6","89xtkJwZ18nB","67NQN5KX2AMe","Iwf50b-R2tYG","GMQiZwjn3iu7","WVIkgGqN3qsr","XkPnILGE3zoT","Hlsf0x5436Go","mT9DMSJo4nBL","c49ITxTc407N","OeJFEK0N496M","9ExmJH0g5HBk","cJNqERVU536h","k5UmGsbsOxih","T0VqWOYE6DLQ","qBMux9mC6MCf","-oLEiFgy-5Pf","C74aWNz2AliB","2DejudWSA-a0","pEMng2IbBLp7","rAdphbQ9Bhjc","TNVZ9zx19K6k","nqoHp30x9hH9","rMDnDkt2B6du","yiiVWRdJDDil","1UUpS68QDMuG","kexQrXU-DjzY","T5CmagL3EC8N","BhH2vgX9EjGr","qjKvONjwE8ra","P1XJ9OREExlT","VFOzZv6IFROw","TIqpNgepFxVj","VfCC591jGiD4","OB4l2ZhMeS1U","ArJBuiUVfxKd","4qY1EAkEfxKe","PiV4Ypx8fxKe","TfvqoZmBfxKf","dJ2tPlVmpsJ0","JWYfwnehpsJ1","-jK_YjpMpsJ2","HAih1iBOpsJ2","zVGeBEFhpsJ2","bmKjuQ-FpsJ3","Fze-IPXLpx6K","7AN1z2sKpx6M","9PIHJqyupx6M","_-qAgymDpx6N","Z-hykwinpx6N","h_CCil-SKHpo","cBFFvTBNJzUa","HvGl1hHyA_VK","EyNgTHvd2WFk","KH5McJBi2d8v","iW_Lq9qf2h6X","-Kee-DAl2viO","gCX9965dhzqZ","gIfDvo9L0UH2"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Project Name**    -\n","\n"],"metadata":{"id":"vncDsAP0Gaoa"}},{"cell_type":"markdown","source":["##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n","##### **Contribution**    - Individual"],"metadata":{"id":"beRrZCGUAJYm"}},{"cell_type":"markdown","source":["# **Project Summary -**"],"metadata":{"id":"FJNUwmbgGyua"}},{"cell_type":"markdown","source":["This project aims to analyze a retail transaction dataset to derive actionable insights that can enhance business strategies, optimize sales processes, and improve operational efficiency. The dataset includes critical information such as invoice numbers, product details, quantities, prices, customer IDs, and country of origin, offering a comprehensive view of each transaction.\n","\n","The primary objective of the analysis is to identify key trends in sales, such as seasonal fluctuations, best-selling products, and customer purchasing behavior. By analyzing the InvoiceDate and Quantity, we can uncover patterns that help forecast demand, optimize inventory levels, and improve stock management strategies.\n","\n","In addition to product performance, the analysis will focus on CustomerID and Country to segment customers based on their purchasing habits, loyalty, and geographic location. This will allow the business to develop targeted marketing campaigns, tailor product offerings to specific regions, and foster stronger customer relationships.\n","\n","By evaluating UnitPrice and total invoice value, we can also identify high-value transactions and uncover pricing strategies that maximize revenue. Furthermore, insights into customer behavior and purchasing frequency will enable personalized promotions and discounts to increase customer retention and lifetime value.\n","\n","Ultimately, this analysis will empower the business to make data-driven decisions that improve profitability, streamline operations, and strengthen its competitive position in the retail market.\n","\n"],"metadata":{"id":"F6v_1wHtG2nS"}},{"cell_type":"markdown","source":["# **GitHub Link -**"],"metadata":{"id":"w6K7xa23Elo4"}},{"cell_type":"markdown","source":["Provide your GitHub Link here."],"metadata":{"id":"h1o69JH3Eqqn"}},{"cell_type":"markdown","source":["# **Problem Statement**\n"],"metadata":{"id":"yQaldy8SH6Dl"}},{"cell_type":"markdown","source":["Analyze a retail transaction dataset to identify trends in sales, customer behavior, product performance, and geographical patterns. Key goals include understanding sales trends, identifying top-performing products, segmenting customers, and calculating invoice values to optimize business strategies and improve decision-making.\n","\n"],"metadata":{"id":"DpeJGUA3kjGy"}},{"cell_type":"markdown","source":["# **General Guidelines** : -  "],"metadata":{"id":"mDgbUHAGgjLW"}},{"cell_type":"markdown","source":["1.   Well-structured, formatted, and commented code is required.\n","2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n","     \n","     The additional credits will have advantages over other students during Star Student selection.\n","       \n","             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n","                       without a single error logged. ]\n","\n","3.   Each and every logic should have proper comments.\n","4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n","        \n","\n","```\n","# Chart visualization code\n","```\n","            \n","\n","*   Why did you pick the specific chart?\n","*   What is/are the insight(s) found from the chart?\n","* Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason.\n","\n","5. You have to create at least 15 logical & meaningful charts having important insights.\n","\n","\n","[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n","\n","U - Univariate Analysis,\n","\n","B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n","\n","M - Multivariate Analysis\n"," ]\n","\n","\n","\n","\n","\n","6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n","\n","\n","*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n","\n","\n","*   Cross- Validation & Hyperparameter Tuning\n","\n","*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n","\n","*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"ZrxVaUj-hHfC"}},{"cell_type":"markdown","source":["# ***Let's Begin !***"],"metadata":{"id":"O_i_v8NEhb9l"}},{"cell_type":"markdown","source":["## ***1. Know Your Data***"],"metadata":{"id":"HhfV-JJviCcP"}},{"cell_type":"markdown","source":["### Import Libraries"],"metadata":{"id":"Y3lxredqlCYt"}},{"cell_type":"code","source":["# Import Libraries\n","import pandas as pd\n","import numpy as np"],"metadata":{"id":"M8Vqi-pPk-HR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Loading"],"metadata":{"id":"3RnN4peoiCZX"}},{"cell_type":"code","source":["# Load Dataset\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"4CkvbW_SlZ_R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset First View"],"metadata":{"id":"x71ZqKXriCWQ"}},{"cell_type":"code","source":["# Dataset First Look\n","df=pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Online Retail.xlsx - Online Retail.csv\")"],"metadata":{"id":"LWNFOSvLl09H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"id":"17DOV2oCUOk6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.columns"],"metadata":{"id":"CH2gb4zxIuqo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bKgBi35PJCU3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Rows & Columns count"],"metadata":{"id":"7hBIi_osiCS2"}},{"cell_type":"code","source":["# Dataset Rows & Columns count\n","import pandas as pd\n","\n","# Load the dataset (replace 'your_dataset.csv' with your actual file)\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Online Retail.xlsx - Online Retail.csv')\n","\n","# Get the number of rows and columns\n","rows, columns = df.shape\n","print(f\"Rows: {rows}, Columns: {columns}\")\n"],"metadata":{"id":"Kllu7SJgmLij"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dataset Information"],"metadata":{"id":"JlHwYmJAmNHm"}},{"cell_type":"code","source":["# Dataset Info\n","#  Summary Statistics for Numerical Columns\n","print(\"\\nSummary Statistics:\")\n","print(df.describe())\n","\n","# 4. Missing Data\n","print(\"\\nMissing Data:\")\n","print(df.isnull().sum())\n","\n","# 5. Unique Values in Each Column\n","print(\"\\nUnique Values in Each Column:\")\n","print(df.nunique())\n","\n","# 6. First Few Rows (Sample Data)\n","print(\"\\nFirst Few Rows:\")\n","print(df.head())"],"metadata":{"id":"e9hRXRi6meOf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# #### Duplicate Values"],"metadata":{"id":"35m5QtbWiB9F"}},{"cell_type":"code","source":["# Dataset Duplicate Value Count\n","duplicates = df[df.duplicated()]\n","\n","# Display duplicates (if any)\n","print(duplicates)\n"],"metadata":{"id":"1sLdpKYkmox0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Missing Values/Null Values"],"metadata":{"id":"PoPl-ycgm1ru"}},{"cell_type":"code","source":["\n","missing_values_count = df.isnull().sum()\n","\n","# Display the missing values count for each column\n","print(\"Missing values count in each column:\")\n","print(missing_values_count)"],"metadata":{"id":"GgHWkxvamxVg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Visualizing the missing values**"],"metadata":{"id":"s0ooylveYc4n"}},{"cell_type":"code","source":["# Visualizing the missing values\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Load the dataset\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Online Retail.xlsx - Online Retail.csv')\n","\n","# Create a heatmap to visualize missing values\n","plt.figure(figsize=(10, 6))\n","sns.heatmap(df.isnull(), cbar=False, cmap='viridis', annot=False, xticklabels=df.columns, yticklabels=False)\n","plt.title('Missing Values Heatmap')\n","plt.show()\n"],"metadata":{"id":"3q5wnI3om9sJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Count the number of missing values per column**"],"metadata":{"id":"leycT7N_Yitp"}},{"cell_type":"code","source":["# Count the number of missing values per column\n","missing_values_count = df.isnull().sum()\n","\n","# Plot the missing values as a bar plot\n","plt.figure(figsize=(10, 6))\n","missing_values_count.plot(kind='bar', color='salmon')\n","plt.title('Missing Values Count per Column')\n","plt.xlabel('Columns')\n","plt.ylabel('Number of Missing Values')\n","plt.xticks(rotation=45)\n","plt.show()\n"],"metadata":{"id":"jfkdslzPXIhL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What did you know about your dataset?\n","Answer Transaction-Based:\n","\n","Each row represents a line item in an invoice. Multiple rows can share the same InvoiceNo if the same customer purchased multiple items in one transaction.\n","Sales Information:\n","\n","The StockCode and Description describe the product being purchased.\n","Quantity and UnitPrice allow us to calculate the total sales value for each item purchased.\n","\n","Customer and Country Data:\n","\n","CustomerID helps identify the customer making the purchase.\n","Country indicates where the customer is from, which is useful for regional analysis.\n","\n","Time-Based Analysis:\n","InvoiceDate allows time-based analysis, such as identifying peak shopping times, seasonality, or trends in sales.\n","\n","\n"],"metadata":{"id":"H0kj-8xxnORC"}},{"cell_type":"markdown","source":["## ***2. Understanding Your Variables***"],"metadata":{"id":"nA9Y7ga8ng1Z"}},{"cell_type":"code","source":["# Dataset Columns\n","df.columns"],"metadata":{"id":"j7xfkqrt5Ag5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dataset Describe\n","df.describe()"],"metadata":{"id":"DnOaZdaE5Q5t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Variables Description"],"metadata":{"id":"PBTbrJXOngz2"}},{"cell_type":"markdown","source":["**InvoiceNo:**\n","\n","A unique identifier for each invoice or order. Each transaction or purchase is associated with an invoice number.\n","\n","**StockCode:**\n","\n","A unique identifier for each product (SKU). It helps track individual items sold in the store.\n","\n","**Description:**\n","\n","A textual description of the product. This provides information about what the product is, like \"WHITE HANGING HEART T-LIGHT HOLDER\" or \"RED WOOLLY HOTTIE WHITE HEART.\"\n","\n","**Quantity:**\n","\n","The number of units of the product purchased in that particular transaction. Can be positive (purchase) or negative (return/cancellation).\n","\n","**InvoiceDate:**\n","\n","The exact date and time when the transaction occurred. This helps track when a sale or purchase was made, typically formatted as \"day/month/year hour:minute.\"\n","\n","**UnitPrice:**\n","\n","The price of one unit of the product at the time of purchase. This varies between products and can sometimes be negative, indicating price errors or refunds.\n","\n","**CustomerID**:\n","\n","A unique identifier for the customer who made the purchase. It helps track individual customers and analyze purchasing behavior. Some transactions may lack a customer ID (indicating an anonymous or one-time customer).\n","\n","**Country:**\n","\n","The country where the customer is located or where the transaction was made. It shows the geographical distribution of your customers, e.g., \"United Kingdom\" or \"France.\""],"metadata":{"id":"aJV4KIxSnxay"}},{"cell_type":"markdown","source":["# Check Unique Values for each variable."],"metadata":{"id":"u3PMJOP6ngxN"}},{"cell_type":"markdown","source":[],"metadata":{"id":"2xTjVPKqY1yU"}},{"cell_type":"markdown","source":[],"metadata":{"id":"z3VPnyXIY1uu"}},{"cell_type":"code","source":["# Check Unique Values for each variable.\n","# List the unique values for each column\n","print(\"\\nUnique Values in Each Column:\")\n","for column in df.columns:\n","    print(f\"\\nUnique values in '{column}':\")\n","    print(df[column].unique())\n"],"metadata":{"id":"zms12Yq5n-jE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. ***Data Wrangling***"],"metadata":{"id":"dauF4eBmngu3"}},{"cell_type":"markdown","source":["### Data Wrangling Code"],"metadata":{"id":"bKJF3rekwFvQ"}},{"cell_type":"code","source":["# Write your code to make your dataset analysis ready.\n","import pandas as pd\n","import numpy as np\n","\n","# Load the dataset\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Online Retail.xlsx - Online Retail.csv')  # Replace with the correct file path\n","\n","# 1. Inspect the dataset\n","print(df.info())  # Check data types, missing values, and structure\n","print(df.head())  # Display first 5 rows\n","\n","# 2. Handle missing values\n","# - CustomerID: Fill missing values with a placeholder, such as 'Unknown' or drop rows\n","df['CustomerID'].fillna('Unknown', inplace=True)\n","# - Description: Drop rows with missing descriptions if critical for analysis\n","df['Description'].dropna(inplace=True)\n","\n","# 3. Convert 'InvoiceDate' to datetime\n","df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], errors='coerce')  # Converts invalid dates to NaT\n","\n","# 4. Remove duplicates\n","df.drop_duplicates(inplace=True)\n","\n","# 5. Handle negative values in 'Quantity' and 'UnitPrice'\n","df = df[df['Quantity'] > 0]  # Remove rows with negative or zero quantity\n","df = df[df['UnitPrice'] > 0]  # Remove rows with negative or zero unit price\n","\n","# 6. Handle categorical columns (e.g., StockCode, Country)\n","# For example, we can encode 'Country' with Label Encoding (for machine learning purposes)\n","df['Country'] = df['Country'].astype('category')\n","df['Country'] = df['Country'].cat.codes  # Converts the categories to numeric codes\n","\n","# You can also perform one-hot encoding for columns like 'StockCode' if needed\n","# df = pd.get_dummies(df, columns=['StockCode'], drop_first=True)\n","\n","# 7. Feature selection or creation (optional)\n","# If needed, you can create new features, for example, extracting the year or month from 'InvoiceDate'\n","df['Year'] = df['InvoiceDate'].dt.year\n","df['Month'] = df['InvoiceDate'].dt.month\n","\n","# 8. Check for any remaining missing values\n","print(df.isnull().sum())  # To ensure that no missing values remain after cleaning\n","\n","# 9. Save the cleaned data to a new CSV (optional)\n","df.to_csv('cleaned_invoice_data.csv', index=False)\n","\n","# The dataset is now ready for analysis!\n"],"metadata":{"id":"wk-9a2fpoLcV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**What all manipulations have you done and insights you found?**"],"metadata":{"id":"MSa1f5Uengrz"}},{"cell_type":"markdown","source":[],"metadata":{"id":"Z1c6eoXpY2OZ"}},{"cell_type":"markdown","source":["Handling Missing Values:\n","\n","Filled missing CustomerID values with 'Unknown' to retain all records.\n","Dropped rows with missing Description values, assuming descriptions are essential for analysis.\n","Datetime Conversion:\n","\n","Converted InvoiceDate to a datetime format to facilitate time-based analysis.\n","Removing Duplicates:\n","\n","Eliminated duplicate rows to ensure data integrity.\n","Handling Negative Values:\n","\n","Removed rows where Quantity or UnitPrice were zero or negative, as these may indicate errors.\n","Categorical Encoding:\n","\n","Label-encoded the Country column to convert categorical data into numerical format for analysis.\n","Feature Engineering:\n","\n","Extracted Year and Month from InvoiceDate to enable time-based analysis.\n","These steps are standard in data preprocessing to ensure the dataset is clean and suitable for analysis. For a more detailed guide on data cleaning and preparation for retail sales data, you can refer to this resource."],"metadata":{"id":"LbyXE7I1olp8"}},{"cell_type":"markdown","source":["## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"],"metadata":{"id":"GF8Ens_Soomf"}},{"cell_type":"markdown","source":["#### Chart - 1"],"metadata":{"id":"0wOQAZs5pc--"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Count the number of invoices per country\n","invoice_counts = df['Country'].value_counts()\n","\n","# Plot\n","invoice_counts.plot(kind='bar')\n","plt.title('Number of Invoices per Country')\n","plt.xlabel('Country')\n","plt.ylabel('Number of Invoices')\n","plt.xticks(rotation=90)\n","plt.show()\n"],"metadata":{"id":"-DY6Ssl9ZvPq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"K5QZ13OEpz2H"}},{"cell_type":"markdown","source":["I chose a bar chart because it effectively compares the number of invoices across different countries. It’s clear, easy to interpret, and works well for categorical data like country names."],"metadata":{"id":"XESiWehPqBRc"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"lQ7QKXXCp7Bj"}},{"cell_type":"markdown","source":["The chart reveals which countries have the highest and lowest number of invoices, helping identify key markets and areas with lower sales activity."],"metadata":{"id":"C_j1G7yiqdRP"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"448CDAPjqfQr"}},{"cell_type":"markdown","source":["The insights can help target high-performing markets and improve underperforming regions, driving growth. However, over-reliance on a few countries could risk negative impact if those markets face disruptions.\n","\n","\n","\n"],"metadata":{"id":"3cspy4FjqxJW"}},{"cell_type":"markdown","source":["#### Chart - 2"],"metadata":{"id":"KSlN3yHqYklG"}},{"cell_type":"code","source":["# Chart - 2 visualization code\n","# Convert 'InvoiceDate' to datetime\n","df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n","\n","# Group by date and sum 'Quantity'\n","daily_sales = df.groupby(df['InvoiceDate'].dt.date)['Quantity'].sum()\n","\n","# Plot\n","daily_sales.plot(kind='line')\n","plt.title('Daily Sales Quantity')\n","plt.xlabel('Date')\n","plt.ylabel('Quantity Sold')\n","plt.xticks(rotation=45)\n","plt.show()\n"],"metadata":{"id":"R4YgtaqtYklH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" 1. Why did you pick the specific chart?"],"metadata":{"id":"t6dVpIINYklI"}},{"cell_type":"markdown","source":["A line chart was chosen because it effectively shows trends and patterns in time-series data (daily sales), making it easy to track changes over time."],"metadata":{"id":"5aaW0BYyYklI"}},{"cell_type":"markdown","source":["2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"ijmpgYnKYklI"}},{"cell_type":"markdown","source":["\n","The chart reveals sales trends, peak days, and patterns, helping optimize inventory, marketing, and sales forecasting.\n","\n","\n","\n"],"metadata":{"id":"PSx9atu2YklI"}},{"cell_type":"markdown","source":[" 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"-JiQyfWJYklI"}},{"cell_type":"markdown","source":["The insights can help optimize marketing and inventory for growth. However, consistent sales dips could indicate issues that, if not addressed, may lead to negative growth."],"metadata":{"id":"BcBbebzrYklV"}},{"cell_type":"markdown","source":["#### Chart - 3"],"metadata":{"id":"EM7whBJCYoAo"}},{"cell_type":"code","source":["# Chart - 3 visualization code\n","# Count the number of invoices per country\n","invoice_counts = df['Country'].value_counts()\n","\n","# Plot\n","invoice_counts.plot(kind='pie', autopct='%1.1f%%', figsize=(8, 8))\n","plt.title('Invoice Distribution by Country')\n","plt.ylabel('')\n","plt.show()\n"],"metadata":{"id":"t6GMdE67YoAp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"fge-S5ZAYoAp"}},{"cell_type":"markdown","source":["I chose a pie chart because it's ideal for showing proportions and relative shares of invoices from different countries, making it easy to compare their contributions visually.\n","\n","\n","\n"],"metadata":{"id":"5dBItgRVYoAp"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"85gYPyotYoAp"}},{"cell_type":"markdown","source":["The chart reveals dominant and underrepresented countries in invoice distribution, helping identify key markets and potential areas for expansion.\n","\n","\n","\n"],"metadata":{"id":"4jstXR6OYoAp"}},{"cell_type":"markdown","source":[" 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"RoGjAbkUYoAp"}},{"cell_type":"markdown","source":["The insights can help focus on key markets for growth and target underrepresented countries. However, over-reliance on a few countries could risk negative growth if those markets face issues.\n","\n","\n","\n"],"metadata":{"id":"zfJ8IqMcYoAp"}},{"cell_type":"markdown","source":["#### Chart - 4"],"metadata":{"id":"4Of9eVA-YrdM"}},{"cell_type":"code","source":["# Chart - 4 visualization code\n","import seaborn as sns\n","\n","# Plot\n","sns.scatterplot(data=df, x='Quantity', y='UnitPrice')\n","plt.title('Quantity vs UnitPrice')\n","plt.xlabel('Quantity')\n","plt.ylabel('UnitPrice')\n","plt.show()\n"],"metadata":{"id":"irlUoxc8YrdO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"iky9q4vBYrdO"}},{"cell_type":"markdown","source":["The scatter plot shows the relationship between Quantity sold and UnitPrice, helping identify if there's any correlation between the number of items sold and their price.\n","\n","\n","\n"],"metadata":{"id":"aJRCwT6DYrdO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"F6T5p64dYrdO"}},{"cell_type":"markdown","source":["The chart shows the relationship between quantity sold and price, highlighting pricing strategies, sales patterns, and potential outliers.\n","\n","\n","\n"],"metadata":{"id":"Xx8WAJvtYrdO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"y-Ehk30pYrdP"}},{"cell_type":"markdown","source":["The insights can optimize pricing strategies for better sales. However, underpricing or poor sales of high-priced items could lead to negative growth if not addressed.\n","\n","\n","\n"],"metadata":{"id":"jLNxxz7MYrdP"}},{"cell_type":"markdown","source":["#### Chart - 5"],"metadata":{"id":"bamQiAODYuh1"}},{"cell_type":"code","source":["# Chart - 5 visualization code\n","# Plot\n","df['Quantity'].plot(kind='hist', bins=50, alpha=0.7)\n","plt.title('Distribution of Quantity')\n","plt.xlabel('Quantity')\n","plt.ylabel('Frequency')\n","plt.show()\n"],"metadata":{"id":"TIJwrbroYuh3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"QHF8YVU7Yuh3"}},{"cell_type":"markdown","source":["  I chose a histogram to visualize the distribution of quantities sold, making it easy to identify patterns, frequency, and potential outliers in the sales data."],"metadata":{"id":"dcxuIMRPYuh3"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"GwzvFGzlYuh3"}},{"cell_type":"markdown","source":["The histogram reveals the most common quantities sold, any skew in sales, and potential outliers, helping optimize inventory and pricing strategies.\n","\n","\n","\n"],"metadata":{"id":"uyqkiB8YYuh3"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"qYpmQ266Yuh3"}},{"cell_type":"markdown","source":["The insights help optimize inventory and sales strategies, but low sales of larger quantities may require adjustments to avoid negative growth."],"metadata":{"id":"_WtzZ_hCYuh4"}},{"cell_type":"markdown","source":["#### Chart - 6"],"metadata":{"id":"OH-pJp9IphqM"}},{"cell_type":"code","source":["# Chart - 6 visualization code\n","# Plot\n","sns.boxplot(data=df, x='Country', y='UnitPrice')\n","plt.title('UnitPrice Distribution by Country')\n","plt.xlabel('Country')\n","plt.ylabel('UnitPrice')\n","plt.xticks(rotation=90)\n","plt.show()\n"],"metadata":{"id":"kuRf4wtuphqN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","```\n","# This is formatted as code\n","```\n","\n","##### 1. Why did you pick the specific chart?"],"metadata":{"id":"bbFf2-_FphqN"}},{"cell_type":"markdown","source":["I chose a boxplot to compare unit price distributions across countries, showing the spread, median, and outliers in the data for each country.\n","\n","\n","\n"],"metadata":{"id":"loh7H2nzphqN"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"_ouA3fa0phqN"}},{"cell_type":"markdown","source":["The boxplot reveals price variation, outliers, and consistency in pricing across countries, helping optimize pricing strategies.\n","\n","\n","\n"],"metadata":{"id":"VECbqPI7phqN"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"Seke61FWphqN"}},{"cell_type":"markdown","source":["The insights can optimize pricing strategies and target premium markets. However, inconsistent or extreme prices may harm customer trust and lead to negative growth.\n","\n","\n","\n"],"metadata":{"id":"DW4_bGpfphqN"}},{"cell_type":"markdown","source":["#### Chart - 7"],"metadata":{"id":"PIIx-8_IphqN"}},{"cell_type":"code","source":["# Chart - 7 visualization code\n","# Group by date and sum 'Quantity'\n","daily_sales = df.groupby(df['InvoiceDate'].dt.date)['Quantity'].sum()\n","\n","# Plot\n","daily_sales.plot(kind='area', alpha=0.5)\n","plt.title('Daily Sales Quantity')\n","plt.xlabel('Date')\n","plt.ylabel('Quantity Sold')\n","plt.xticks(rotation=45)\n","plt.show()\n"],"metadata":{"id":"lqAIGUfyphqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"t27r6nlMphqO"}},{"cell_type":"markdown","source":["The area chart shows the total quantity sold each day, highlighting trends and fluctuations in sales over time."],"metadata":{"id":"iv6ro40sphqO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"r2jJGEOYphqO"}},{"cell_type":"markdown","source":["The chart reveals sales trends, peak sales periods, and sales dips, helping guide marketing and inventory decisions."],"metadata":{"id":"Po6ZPi4hphqO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"b0JNsNcRphqO"}},{"cell_type":"markdown","source":["The insights help optimize strategies during peak sales, but consistent sales dips may indicate issues that could lead to negative growth if not addressed.\n","\n","\n","\n"],"metadata":{"id":"xvSq8iUTphqO"}},{"cell_type":"markdown","source":["#### Chart - 8"],"metadata":{"id":"BZR9WyysphqO"}},{"cell_type":"code","source":["# Chart - 8 visualization code\n","# Pivot table for heatmap\n","pivot_table = df.pivot_table(values='Quantity', index='Country', columns='InvoiceDate', aggfunc='sum')\n","\n","# Plot\n","sns.heatmap(pivot_table, cmap='YlGnBu', annot=False)\n","plt.title('Heatmap of Sales Quantity by Country and Date')\n","plt.xlabel('Date')\n","plt.ylabel('Country')\n","plt.show()\n"],"metadata":{"id":"TdPTWpAVphqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"jj7wYXLtphqO"}},{"cell_type":"markdown","source":["I chose a heatmap to easily visualize sales patterns across countries and dates, highlighting trends and variations through color intensity.\n","\n","\n","\n"],"metadata":{"id":"Ob8u6rCTphqO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"eZrbJ2SmphqO"}},{"cell_type":"markdown","source":["The heatmap reveals sales peaks, seasonal trends, and country-specific patterns, helping to optimize sales strategies and inventory planning.\n","\n","\n","\n"],"metadata":{"id":"mZtgC_hjphqO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"rFu4xreNphqO"}},{"cell_type":"markdown","source":["The insights help optimize sales strategies and focus on regions with high demand. However, irregular sales in some countries could lead to challenges, affecting growth if not managed properly.\n","\n","\n","\n"],"metadata":{"id":"ey_0qi68phqO"}},{"cell_type":"markdown","source":["#### Chart - 9"],"metadata":{"id":"YJ55k-q6phqO"}},{"cell_type":"code","source":["# Chart - 9 visualization code\n","# Group by 'StockCode' and sum 'Quantity'\n","product_sales = df.groupby('StockCode')['Quantity'].sum().reset_index()\n","\n","# Plot\n","plt.figure(figsize=(10, 6))\n","plt.scatter(data=product_sales, x='StockCode', y='Quantity', s=product_sales['Quantity'] / 10, alpha=0.5)\n","plt.title('Product Sales Quantity')\n","plt.xlabel('StockCode')\n","plt.ylabel('Quantity Sold')\n","plt.show()\n"],"metadata":{"id":"B2aS4O1ophqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"gCFgpxoyphqP"}},{"cell_type":"markdown","source":["I  chose a scatter plot to visualize total sales by product and highlight high-selling items using point size for clarity.\n","\n","\n","\n"],"metadata":{"id":"TVxDimi2phqP"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"OVtJsKN_phqQ"}},{"cell_type":"markdown","source":["The scatter plot reveals top-selling products, low-selling products, and the sales distribution across items, helping with inventory and marketing decisions.\n","\n","\n","\n"],"metadata":{"id":"ngGi97qjphqQ"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"lssrdh5qphqQ"}},{"cell_type":"markdown","source":["The insights help optimize inventory and marketing for top-selling products. However, low-selling products need attention to avoid wasted resources and losses.\n","\n","\n","\n"],"metadata":{"id":"tBpY5ekJphqQ"}},{"cell_type":"markdown","source":["#### Chart - 10"],"metadata":{"id":"U2RJ9gkRphqQ"}},{"cell_type":"code","source":["# Chart - 10 visualization code\n","import numpy as np\n","\n","# Example data\n","categories = ['Quantity', 'UnitPrice']\n","values = [df['Quantity'].mean(), df['UnitPrice'].mean()]\n","\n","# Number of variables\n","num_vars = len(categories)\n","\n","# Compute angle for each axis\n","angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n","\n","# Complete the loop\n","values += values[:1]\n","angles += angles[:1]\n","\n","# Plot\n","fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n","ax.fill(angles, values, color='red', alpha=0.25)\n","ax.plot(angles, values, color='red', linewidth=2)\n","ax.set_yticklabels([])\n","ax.set_xticks(angles[:-1])\n","ax.set_xticklabels(categories)\n","plt.title('Radar Chart of Quantity and UnitPrice')\n","plt.show()\n"],"metadata":{"id":"GM7a4YP4phqQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"1M8mcRywphqQ"}},{"cell_type":"markdown","source":["  I chose a radar chart to easily compare the average values of Quantity and UnitPrice on a common scale, making their relationship visually clear.\n","\n","\n","\n"],"metadata":{"id":"8agQvks0phqQ"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"tgIPom80phqQ"}},{"cell_type":"markdown","source":["The radar chart compares the average values of Quantity and UnitPrice, showing their relative balance and helping inform pricing and sales strategies.\n","\n","\n","\n"],"metadata":{"id":"Qp13pnNzphqQ"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"JMzcOPDDphqR"}},{"cell_type":"markdown","source":["The insights help optimize pricing for better sales. However, selling high quantities at low prices could erode profit margins, potentially harming growth.\n","\n","\n","\n"],"metadata":{"id":"R4Ka1PC2phqR"}},{"cell_type":"markdown","source":["#### Chart - 11"],"metadata":{"id":"x-EpHcCOp1ci"}},{"cell_type":"code","source":["# Chart - 11 visualization code\n","# Group by 'InvoiceDate' and 'Country' and sum 'Quantity'\n","stacked_data = df.groupby([df['InvoiceDate'].dt.date, 'Country'])['Quantity'].sum().unstack()\n","\n","# Plot\n","stacked_data.plot(kind='bar', stacked=True, figsize=(10, 6))\n","plt.title('Stacked Bar Chart of Sales Quantity by Country and Date')\n","plt.xlabel('Date')\n","plt.ylabel('Quantity Sold')\n","plt.xticks(rotation=45)\n","plt.show()\n"],"metadata":{"id":"mAQTIvtqp1cj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"X_VqEhTip1ck"}},{"cell_type":"markdown","source":["I chose a stacked bar chart to compare sales quantities across countries over time, making it easy to track contributions and trends.\n","\n","\n","\n"],"metadata":{"id":"-vsMzt_np1ck"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"8zGJKyg5p1ck"}},{"cell_type":"markdown","source":["The insights from the stacked bar chart could include:\n","\n","Sales Distribution by Country: It shows how each country's sales contribute to the total sales on specific dates.\n","Trends Over Time: Identifying which countries have increasing or decreasing sales over time.\n","Top Performing Countries: Spotting which countries consistently perform well and which are lagging behind."],"metadata":{"id":"ZYdMsrqVp1ck"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"PVzmfK_Ep1ck"}},{"cell_type":"markdown","source":["The insights help optimize marketing and resource allocation for top-performing countries. However, underperforming countries may indicate issues that need addressing to avoid negative growth.\n","\n","\n","\n"],"metadata":{"id":"druuKYZpp1ck"}},{"cell_type":"markdown","source":["#### Chart - 12"],"metadata":{"id":"n3dbpmDWp1ck"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Example data: sequential changes in sales\n","categories = ['Start', 'Sales', 'Returns', 'Discounts', 'End']\n","values = [0, 100, -20, -10, 70]\n","\n","# Calculate the cumulative sum\n","cumulative_values = np.cumsum(values)\n","\n","# Create a figure and axis\n","fig, ax = plt.subplots(figsize=(10, 6))\n","\n","# Plot the bars\n","ax.bar(categories, cumulative_values, color=['blue', 'green', 'red', 'orange', 'purple'])\n","\n","# Add labels\n","for i, (category, value) in enumerate(zip(categories, cumulative_values)):\n","    ax.text(i, value, f'{value}', ha='center', va='bottom' if value >= 0 else 'top')\n","\n","# Set titles and labels\n","ax.set_title('Waterfall Chart of Sales Changes')\n","ax.set_xlabel('Category')\n","ax.set_ylabel('Value')\n","\n","plt.show()\n"],"metadata":{"id":"8DBmgfR4ba2d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1. Why did you pick the specific chart?"],"metadata":{"id":"ylSl6qgtp1ck"}},{"cell_type":"markdown","source":["I  chose a waterfall chart to clearly show how each factor (sales, returns, discounts) sequentially impacts the overall result.\n","\n","\n","\n"],"metadata":{"id":"m2xqNkiQp1ck"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"ZWILFDl5p1ck"}},{"cell_type":"markdown","source":["The insights from the waterfall chart could include:\n","\n","Positive and Negative Contributions: It highlights how each factor (sales, returns, discounts) affects the final outcome, showing where gains and losses occur.\n","Impact of Returns and Discounts: Identifying the impact of negative factors like returns and discounts on the final sales value."],"metadata":{"id":"x-lUsV2mp1ck"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"M7G43BXep1ck"}},{"cell_type":"markdown","source":["The insights help optimize sales strategies and improve customer satisfaction. However, high returns and discounts can negatively impact profitability and growth if not managed well.\n","\n","\n","\n"],"metadata":{"id":"5wwDJXsLp1cl"}},{"cell_type":"markdown","source":["#### Chart - 13"],"metadata":{"id":"Ag9LCva-p1cl"}},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Assuming 'df' is your DataFrame\n","plt.figure(figsize=(10, 6))\n","sns.violinplot(data=df, x='Country', y='UnitPrice')\n","plt.title('Distribution of UnitPrice by Country')\n","plt.xlabel('Country')\n","plt.ylabel('UnitPrice')\n","plt.xticks(rotation=90)\n","plt.show()\n"],"metadata":{"id":"Pl3BzLLscfUL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"E6MkPsBcp1cl"}},{"cell_type":"markdown","source":["I chose a violin plot to show the distribution and spread of UnitPrice across countries, providing more detailed insights than a box plot.\n","\n","\n","\n"],"metadata":{"id":"V22bRsFWp1cl"}},{"cell_type":"markdown","source":["##### 2. **What** is/are the insight(s) found from the chart?"],"metadata":{"id":"2cELzS2fp1cl"}},{"cell_type":"markdown","source":["The violin plot shows price variation, identifies outliers, and highlights skewness in UnitPrice across countries, helping to adjust pricing strategies.\n","\n","\n","\n"],"metadata":{"id":"ozQPc2_Ip1cl"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"3MPXvC8up1cl"}},{"cell_type":"markdown","source":["The insights help optimize pricing and target markets better. However, extreme price variations could lead to customer dissatisfaction and negatively impact growth.\n","\n","\n","\n"],"metadata":{"id":"GL8l1tdLp1cl"}},{"cell_type":"markdown","source":["#### Chart - 14 - Correlation Heatmap"],"metadata":{"id":"NC_X3p0fY2L0"}},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Calculate correlation matrix\n","correlation_matrix = df[['Quantity', 'UnitPrice']].corr()\n","\n","# Plot\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n","plt.title('Correlation Heatmap of Quantity and UnitPrice')\n","plt.show()\n"],"metadata":{"id":"NxcxbBs1b-i_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"UV0SzAkaZNRQ"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"DVPuT8LYZNRQ"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"YPEH6qLeZNRQ"}},{"cell_type":"markdown","source":["  I chose a correlation heatmap to visually show the relationship between Quantity and UnitPrice, making it easy to identify the strength and direction of their correlation.\n","\n","\n","\n"],"metadata":{"id":"bfSqtnDqZNRR"}},{"cell_type":"markdown","source":["#### Chart - 15 - Pair Plot"],"metadata":{"id":"q29F0dvdveiT"}},{"cell_type":"code","source":["# Pair Plot visualization code\n","import seaborn as sns\n","\n","# Assuming 'df' is your DataFrame\n","sns.pairplot(df[['Quantity', 'UnitPrice']])\n","plt.suptitle('Pair Plot of Quantity and UnitPrice', y=1.02)\n","plt.show()\n"],"metadata":{"id":"o58-TEIhveiU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"EXh0U9oCveiU"}},{"cell_type":"markdown","source":["I chose a pair plot to visualize the relationship between Quantity and UnitPrice and their individual distributions in a clear, simple way.\n","\n","\n","\n"],"metadata":{"id":"eMmPjTByveiU"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"22aHeOlLveiV"}},{"cell_type":"markdown","source":["The pair plot shows the relationship between Quantity and UnitPrice, their distributions, and potential outliers.\n","\n","\n","\n"],"metadata":{"id":"uPQ8RGwHveiV"}},{"cell_type":"markdown","source":["# 5. **Hypothesis Testing**"],"metadata":{"id":"g-ATYxFrGrvw"}},{"cell_type":"markdown","source":["### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."],"metadata":{"id":"Yfr_Vlr8HBkt"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"-7MS06SUHkB-"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 1"],"metadata":{"id":"8yEUt7NnHlrM"}},{"cell_type":"markdown","source":["1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"tEA2Xm5dHt1r"}},{"cell_type":"markdown","source":["**Null Hypothesis (H₀):**\n","There is no significant difference in the average 'Quantity' purchased between customers from different countries.\n","\n","**Alternative Hypothesis (H₁):**\n","There is a significant difference in the average 'Quantity' purchased between customers from different countries."],"metadata":{"id":"HI9ZP0laH0D-"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"I79__PHVH19G"}},{"cell_type":"markdown","source":["**One-Way Analysis of Variance (ANOVA)**"],"metadata":{"id":"czCWFBlD21U3"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value\n","import pandas as pd\n","from scipy import stats\n","\n","# Assuming 'df' is your DataFrame\n","# Grouping data by 'Country' and extracting 'Quantity' for each group\n","countries = df['Country'].unique()\n","quantities_by_country = [df[df['Country'] == country]['Quantity'] for country in countries]\n","\n","# Performing One-Way ANOVA\n","f_stat, p_value = stats.f_oneway(*quantities_by_country)\n","print(f\"ANOVA F-statistic: {f_stat}, p-value: {p_value}\")\n"],"metadata":{"id":"oZrfquKtyian"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"Ou-I18pAyIpj"}},{"cell_type":"markdown","source":["One-Way Analysis of Variance (ANOVA)"],"metadata":{"id":"s2U0kk00ygSB"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"fF3858GYyt-u"}},{"cell_type":"markdown","source":["ANOVA is appropriate when comparing the means of three or more independent groups to determine if at least one group mean is different from the others. In this case, the groups are customers from different countries, and the objective is to assess if their average 'Quantity' purchased differs significantly"],"metadata":{"id":"HO4K0gP5y3B4"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 2"],"metadata":{"id":"4_0_7-oCpUZd"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"hwyV_J3ipUZe"}},{"cell_type":"markdown","source":["**Null Hypothesis (H₀):**\n","There is no significant relationship between 'UnitPrice' and 'Quantity' purchased.\n","\n","**Alternative Hypothesis (H₁):**\n","There is a significant relationship between 'UnitPrice' and 'Quantity' purchased."],"metadata":{"id":"FnpLGJ-4pUZe"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"3yB-zSqbpUZe"}},{"cell_type":"markdown","source":["**Pearson Correlation Coefficient**"],"metadata":{"id":"LtouRgdV3umQ"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value\n","from scipy import stats\n","\n","# Calculating Pearson Correlation Coefficient\n","correlation, p_value = stats.pearsonr(df['UnitPrice'], df['Quantity'])\n","print(f\"Pearson Correlation: {correlation}, p-value: {p_value}\")\n"],"metadata":{"id":"sWxdNTXNpUZe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"dEUvejAfpUZe"}},{"cell_type":"markdown","source":["Pearson Correlation Coefficient"],"metadata":{"id":"oLDrPz7HpUZf"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"Fd15vwWVpUZf"}},{"cell_type":"markdown","source":["The Pearson Correlation Coefficient measures the strength and direction of the linear relationship between two continuous variables. Since both 'UnitPrice' and 'Quantity' are continuous variables, this test is suitable for assessing their linear association.\n"],"metadata":{"id":"4xOGYyiBpUZf"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 3"],"metadata":{"id":"bn_IUdTipZyH"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"49K5P_iCpZyH"}},{"cell_type":"markdown","source":["**Null Hypothesis (H₀):**\n","The average 'UnitPrice' of products purchased by UK customers is equal to that of French customers.\n","\n","**Alternative Hypothesis (H₁):**\n","The average 'UnitPrice' of products purchased by UK customers is different from that of French customers."],"metadata":{"id":"7gWI5rT9pZyH"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"Nff-vKELpZyI"}},{"cell_type":"markdown","source":["**Independent Two-Sample T-Test**"],"metadata":{"id":"qhmDpm6T4Fgd"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value\n","from scipy import stats\n","\n","# Extracting 'UnitPrice' for UK and France customers\n","uk_prices = df[df['Country'] == 'United Kingdom']['UnitPrice']\n","france_prices = df[df['Country'] == 'France']['UnitPrice']\n","\n","# Performing Independent Two-Sample T-Test\n","t_stat, p_value = stats.ttest_ind(uk_prices, france_prices)\n","print(f\"T-statistic: {t_stat}, p-value: {p_value}\")\n"],"metadata":{"id":"s6AnJQjtpZyI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"kLW572S8pZyI"}},{"cell_type":"markdown","source":["Independent Two-Sample T-Test"],"metadata":{"id":"ytWJ8v15pZyI"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"dWbDXHzopZyI"}},{"cell_type":"markdown","source":["The Independent Two-Sample T-Test compares the means of two independent groups to determine if there is a statistically significant difference between them. Here, the groups are customers from the UK and France, and the goal is to compare their average 'UnitPrice' purchases."],"metadata":{"id":"M99G98V6pZyI"}},{"cell_type":"markdown","source":["## ***6. Feature Engineering & Data Pre-processing***"],"metadata":{"id":"yLjJCtPM0KBk"}},{"cell_type":"markdown","source":["### 1. Handling Missing Values"],"metadata":{"id":"xiyOF9F70UgQ"}},{"cell_type":"code","source":["# Handling Missing Values & Missing Value Imputation\n","import pandas as pd\n","\n","  # Load your dataset\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Online Retail.xlsx - Online Retail.csv')\n","\n","  # Check for missing values\n","missing_data = df.isnull().sum()\n","print(missing_data)\n"],"metadata":{"id":"iRsAHk1K0fpS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### What all missing value imputation techniques have you used and why did you use those techniques?"],"metadata":{"id":"7wuGOrhz0itI"}},{"cell_type":"markdown","source":["Mean/Median Imputation: Replace missing numerical values with the mean or median of the column. This method is simple and effective for normally distributed data.\n","\n","\n","Mode Imputation: Replace missing categorical values with the mode (most frequent value) of the column. This approach is straightforward and works well for categorical data.\n","\n","\n","Forward Fill and Backward Fill: Use the previous (forward fill) or next (backward fill) available value to fill missing entries. This technique is useful for time-series data but assumes that the data points are related."],"metadata":{"id":"1ixusLtI0pqI"}},{"cell_type":"markdown","source":["### 2. Handling Outliers"],"metadata":{"id":"id1riN9m0vUs"}},{"cell_type":"code","source":["# Handling Outliers & Outlier treatments\n","import numpy as np\n","import pandas as pd\n","\n","  # Sample data\n","data = pd.DataFrame(df)\n","\n","  # Calculate Z-scores\n","z_scores = (df['value'] - df['value'].mean()) / df['value'].std()\n","\n","  # Identify outliers\n","threshold = 3\n","outliers = df[np.abs(z_scores) > threshold]\n","print(outliers)\n"],"metadata":{"id":"M6w2CzZf04JK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ##### What all outlier treatment techniques have you used and why did you use those techniques?"],"metadata":{"id":"578E2V7j08f6"}},{"cell_type":"markdown","source":["1 Z-Score Method: Identifies data points that are a specified number of standard deviations away from the mean.\n","\n","2 Interquartile Range (IQR) Method: Detects outliers based on the spread of the middle 50% of the data.\n","\n","3 Visualization Techniques: Utilizes box plots and scatter plots to visually identify outliers.\n","\n","4 Removal of Outliers: Eliminates rows containing outliers to prevent them from skewing analysis results.\n","\n","5 Capping (Winsorizing): Limits outlier values to a specified range to reduce their impact.\n","\n","6 Transformation: Applies mathematical transformations, such as logarithmic or square root transformations, to reduce the effect of outliers.\n","\n","7 Imputation: Replaces outliers with statistical measures like the mean or median to minimize their effect.\n","\n","8 Robust Models: Utilizes algorithms less sensitive to outliers, such as Random Forests or robust regression techniques."],"metadata":{"id":"uGZz5OrT1HH-"}},{"cell_type":"markdown","source":["### 3. Categorical Encoding"],"metadata":{"id":"89xtkJwZ18nB"}},{"cell_type":"markdown","source":["# **Label Encoding**"],"metadata":{"id":"JQ3HAkQkmjf5"}},{"cell_type":"code","source":["# Encode your categorical columns\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Sample data\n","data = {'Country': ['United Kingdom', 'France', 'United Kingdom', 'France']}\n","df = pd.DataFrame(data)\n","\n","# Initialize LabelEncoder\n","le = LabelEncoder()\n","\n","# Apply label encoding\n","df['Country_encoded'] = le.fit_transform(df['Country'])\n","print(df)\n"],"metadata":{"id":"21JmIYMG2hEo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Target Encoding**"],"metadata":{"id":"T3CbIIalj3hG"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Sample data\n","data = {'Country': ['United Kingdom', 'France', 'United Kingdom', 'France'],\n","        'Sales': [100, 200, 150, 250]}\n","df = pd.DataFrame(data)\n","\n","# Calculate mean sales per country\n","mean_sales = df.groupby('Country')['Sales'].mean()\n","\n","# Map the means to the original data\n","df['Country_encoded'] = df['Country'].map(mean_sales)\n","print(df)\n"],"metadata":{"id":"pCnF6Ot1jpLl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **One Hot Encoding**"],"metadata":{"id":"NjtqsExXmuFO"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Sample data\n","data = {'Country': ['United Kingdom', 'France', 'United Kingdom', 'France']}\n","df = pd.DataFrame(data)\n","\n","# Apply one-hot encoding\n","df_encoded = pd.get_dummies(df, columns=['Country'])\n","print(df_encoded)\n"],"metadata":{"id":"nfzvWwFAms6Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### What all categorical encoding techniques have you used & why did you use those techniques?"],"metadata":{"id":"67NQN5KX2AMe"}},{"cell_type":"markdown","source":["1label Encoding: Assigns a unique integer to each category. It's suitable for ordinal variables where the order matters.\n","\n","2-Hot Encoding: Creates a binary column for each category. It's ideal for nominal variables without inherent order.\n","\n","3Target Encoding: Replaces each category with the mean of the target variable for that category. It's effective for high-cardinality features."],"metadata":{"id":"UDaue5h32n_G"}},{"cell_type":"markdown","source":["### 4. Textual Data Preprocessing\n","(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"],"metadata":{"id":"Iwf50b-R2tYG"}},{"cell_type":"markdown","source":["#### 1. Expand Contraction"],"metadata":{"id":"GMQiZwjn3iu7"}},{"cell_type":"code","source":["exit()\n"],"metadata":{"id":"ARkwStN-9mnd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install contractions\n"],"metadata":{"id":"4VdLJ85y93MP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import contractions\n","\n","# Sample text with contractions\n","text = \"I can't believe it's already 5 o'clock. She won't  be here until 6 .\"\n","\n","# Expand contractions\n","expanded_text = contractions.fix(text)\n","\n","print(expanded_text)\n"],"metadata":{"id":"bnbtuvNp99sg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Lower Casing"],"metadata":{"id":"WVIkgGqN3qsr"}},{"cell_type":"code","source":["# Lower Casing\n","# Sample text\n","text = \"Hello, World! This is a Sample Text.\"\n","\n","# Convert text to lowercase\n","lowercase_text = text.lower()\n","\n","print(lowercase_text)\n"],"metadata":{"id":"88JnJ1jN3w7j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3. Removing Punctuations"],"metadata":{"id":"XkPnILGE3zoT"}},{"cell_type":"code","source":["# Remove Punctuations\n","import string\n","\n","# Sample text\n","text = \"Hello, World! This is a sample text.\"\n","\n","# Create a translation table that maps each punctuation character to None\n","translation_table = str.maketrans('', '', string.punctuation)\n","\n","# Remove punctuation\n","text_without_punctuation = text.translate(translation_table)\n","\n","print(text_without_punctuation)\n"],"metadata":{"id":"vqbBqNaA33c0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 4. Removing URLs & Removing words and digits contain digits."],"metadata":{"id":"Hlsf0x5436Go"}},{"cell_type":"code","source":["# Remove URLs & Remove words and digits contain digits\n","import re\n","\n","# Sample text\n","text = \"Visit our website at https://www.example.com for more information.\"\n","\n","# Regular expression pattern to match URLs\n","url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n","\n","# Remove URLs\n","text_without_urls = re.sub(url_pattern, '', text)\n","\n","print(text_without_urls)\n"],"metadata":{"id":"2sxKgKxu4Ip3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 5. Removing Stopwords & Removing White spaces"],"metadata":{"id":"mT9DMSJo4nBL"}},{"cell_type":"code","source":["pip install nltk\n"],"metadata":{"id":"9bvBVpNTB3O9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')\n"],"metadata":{"id":"nipoV-hWCU2j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import spacy\n","\n","# Load the English language model\n","nlp = spacy.load('en_core_web_sm')\n","\n","# Sample text\n","text = \"This is a sample sentence demonstrating stopword removal.\"\n","\n","# Process the text\n","doc = nlp(text)\n","\n","# Remove stopwords\n","filtered_words = [token.text for token in doc if not token.is_stop and not token.is_punct]\n","\n","print(filtered_words)\n"],"metadata":{"id":"iYO92bl0ElqK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove White spaces\n","text = \"  Hello,   World! \\n\\t  \"\n","cleaned_text = text.replace(\" \", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n","print(cleaned_text)  # Output: \"Hello,World!\"\n"],"metadata":{"id":"EgLJGffy4vm0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 6. Rephrase Text"],"metadata":{"id":"c49ITxTc407N"}},{"cell_type":"code","source":["# Rephrase Text"],"metadata":{"id":"foqY80Qu48N2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install transformers torch\n"],"metadata":{"id":"JU6gV-dfFlZv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import T5ForConditionalGeneration, T5Tokenizer\n"],"metadata":{"id":"lti5dV86Fo4s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name = \"t5-base\"  # You can also use \"t5-large\" or \"t5-small\" based on your requirements\n","model = T5ForConditionalGeneration.from_pretrained(model_name)\n","tokenizer = T5Tokenizer.from_pretrained(model_name)\n"],"metadata":{"id":"t9zck9OlFt41"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def paraphrase(text):\n","    # Prepend the text with a task-specific prefix\n","    input_text = f\"paraphrase: {text}\"\n","    # Tokenize the input text\n","    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n","    # Generate paraphrased text\n","    outputs = model.generate(input_ids, max_length=512, num_beams=5, num_return_sequences=1, early_stopping=True)\n","    # Decode the generated text\n","    paraphrased_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    return paraphrased_text\n"],"metadata":{"id":"hf_E1Y4CF0AX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["original_text = \"Your original text goes here.\"\n","paraphrased_text = paraphrase(original_text)\n","print(\"Original Text:\", original_text)\n","print(\"Paraphrased Text:\", paraphrased_text)\n"],"metadata":{"id":"TJQpW5TlF2Ie"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 7. Tokenization"],"metadata":{"id":"OeJFEK0N496M"}},{"cell_type":"code","source":["exit()\n"],"metadata":{"id":"yxzmt-joH2zG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenization\n","pip install nltk\n"],"metadata":{"id":"ijx1rUOS5CUU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n"],"metadata":{"id":"a8deewzeLQUI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","\n","text = \"Hello, world! Welcome to NLP with Python.\"\n","words = word_tokenize(text)\n","print(words)\n"],"metadata":{"id":"0Eih586qLZhs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 8. Text Normalization"],"metadata":{"id":"9ExmJH0g5HBk"}},{"cell_type":"code","source":["# Normalizing Text (i.e., Stemming, Lemmatization etc.)"],"metadata":{"id":"AIJ1a-Zc5PY8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","\n","# Download the punkt resource again\n","nltk.download('punkt')\n"],"metadata":{"id":"7wvEIdtyiFmE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('punkt_tab')\n"],"metadata":{"id":"BG_Wyc-RiaCu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","import string\n","\n","# Download NLTK resources\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","# Sample text\n","text = \"The cats are running quickly through the streets.\"\n","\n","# Step 1: Lowercase the text\n","text = text.lower()\n","\n","# Step 2: Remove punctuation\n","text = text.translate(str.maketrans('', '', string.punctuation))\n","\n","# Step 3: Tokenize the text (still necessary to split words for stopword removal and stemming/lemmatization)\n","tokens = word_tokenize(text)\n","\n","# Step 4: Remove stopwords\n","stop_words = set(stopwords.words('english'))\n","filtered_tokens = [word for word in tokens if word not in stop_words]\n","\n","# Step 5: Apply Stemming (PorterStemmer)\n","porter_stemmer = PorterStemmer()\n","stemmed_tokens = [porter_stemmer.stem(word) for word in filtered_tokens]\n","\n","# Step 6: Apply Lemmatization (using WordNet Lemmatizer)\n","lemmatizer = WordNetLemmatizer()\n","lemmatized_tokens = [lemmatizer.lemmatize(word, pos='v') for word in filtered_tokens]  # Lemmatize verbs\n","\n","# Display the results\n","print(\"Original Text:\", text)\n","print(\"Filtered Tokens (No Stopwords):\", filtered_tokens)\n","print(\"Stemmed Tokens:\", stemmed_tokens)\n","print(\"Lemmatized Tokens:\",lemmatized_tokens)\n","\n"],"metadata":{"id":"33QwLH3vgaJG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which text normalization technique have you used and why?"],"metadata":{"id":"cJNqERVU536h"}},{"cell_type":"markdown","source":["\n","\n","1. Lowercasing:\n","Definition: Lowercasing is the process of converting all characters in the text to lowercase.\n","Why: It helps in ensuring that words like \"The\" and \"the\" are treated as identical. This is crucial in NLP tasks to avoid distinguishing between words that are essentially the same but appear in different cases. For instance, \"running\" and \"Running\" should be recognized as the same word during analysis.\n","2. Tokenization:\n","Definition: Tokenization involves splitting the raw text into smaller units, called tokens. These tokens could be words, sub-words, or sentences.\n","Why: Tokenization is necessary for further analysis because raw text in the form of a long sentence isn’t easily analyzable. By splitting the sentence into individual tokens, you can apply further processing like part-of-speech tagging, named entity recognition, or sentiment analysis.\n","3. Removing Punctuation:\n","Definition: Removing punctuation marks like commas, periods, quotation marks, exclamation marks, etc., from the text.\n","Why: Punctuation often doesn't contribute significantly to the meaning in many NLP tasks. For example, in sentiment analysis, the words themselves hold more meaning than the punctuation surrounding them. Therefore, removing punctuation simplifies the text and reduces noise.\n","4. Stopword Removal:\n","Definition: Stopwords are common words such as \"is\", \"the\", \"and\", \"in\", \"of\", etc., which do not carry much useful information for tasks like classification or information retrieval.\n","Why: These words appear frequently in text but don’t help in differentiating between topics or sentiments. Removing stopwords improves efficiency and can help focus on the more meaningful content of the text. For example, in a document classification task, the word \"the\" wouldn't help to distinguish between topics.\n","5. Stemming:\n","Definition: Stemming is the process of reducing words to their root form by chopping off derivational affixes (e.g., removing \"-ing\", \"-ly\", \"-ed\", etc.).\n","Why: Stemming allows us to treat different forms of a word as the same base word. For example, \"running\" and \"runner\" would both be reduced to \"run\", simplifying analysis. However, stemming is a heuristic process and may sometimes result in non-dictionary forms (e.g., \"quickli\" instead of \"quickly\").\n","6. Lemmatization:\n","Definition: Lemmatization also reduces words to their root form, but it considers the word’s part of speech (e.g., whether it’s a verb, noun, or adjective). Unlike stemming, lemmatization uses vocabulary and grammatical analysis to ensure that the word is reduced to a valid dictionary form.\n","Why: Lemmatization is more sophisticated and accurate than stemming, as it returns the actual base form of a word. For example, \"running\" becomes \"run\", and \"better\" becomes \"good\". Lemmatization ensures the reduced form is a meaningful word"],"metadata":{"id":"Z9jKVxE06BC1"}},{"cell_type":"markdown","source":["# 9. **Part of speech tagging**"],"metadata":{"id":"k5UmGsbsOxih"}},{"cell_type":"code","source":["# POS Taging"],"metadata":{"id":"btT3ZJBAO6Ik"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","\n","# Download the POS tagger manually\n","nltk.download('averaged_perceptron_tagger')\n"],"metadata":{"id":"ddN2guojjwEb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","\n","# Clear the NLTK cache (use cautiously if you have other downloaded resources)\n","nltk.data.clear_cache()\n","\n","# Try downloading the necessary resources again\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n"],"metadata":{"id":"YBErQwEukO0-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","\n","# Specify the path where resources are stored\n","nltk.data.path.append('/root/nltk_data')\n","\n","# Re-download the necessary resources\n","nltk.download('averaged_perceptron_tagger')\n"],"metadata":{"id":"oNYMg8uhj1of"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 10. Text Vectorization"],"metadata":{"id":"T0VqWOYE6DLQ"}},{"cell_type":"code","source":["# Vectorizing Text"],"metadata":{"id":"yBRtdhth6JDE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Sample text data\n","documents = [\"I love programming\", \"Programming is fun\", \"I love Python programming\"]\n","\n","# Create the CountVectorizer (BoW)\n","vectorizer = CountVectorizer()\n","\n","# Fit and transform the data into the BoW model\n","X = vectorizer.fit_transform(documents)\n","\n","# Display the vocabulary and the corresponding vectors\n","print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n","print(\"BoW Matrix:\\n\", X.toarray())\n"],"metadata":{"id":"sd6NqOWxl9M_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which text vectorization technique have you used and why?"],"metadata":{"id":"qBMux9mC6MCf"}},{"cell_type":"markdown","source":[" I used the Bag of Words (BoW) technique because it's simple, fast, and effective for text classification tasks. It focuses on the frequency of words in documents, which is useful when the order of words isn't important. However, it doesn't capture word relationships or semantic meaning.\n","\n","\n","\n"],"metadata":{"id":"su2EnbCh6UKQ"}},{"cell_type":"markdown","source":["### 4. Feature Manipulation & Selection"],"metadata":{"id":"-oLEiFgy-5Pf"}},{"cell_type":"markdown","source":["#### 1. Feature Manipulation"],"metadata":{"id":"C74aWNz2AliB"}},{"cell_type":"code","source":["df = pd.get_dummies(df, columns=['StockCode', 'Country'], drop_first=True)\n"],"metadata":{"id":"a_t0rrV0fB-S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Manipulate Features to minimize feature correlation and create new features\n","# Calculate correlation matrix\n","corr_matrix = df.corr()\n","# Find columns with correlation higher than 0.9\n","high_corr = corr_matrix[corr_matrix.abs() > 0.9]\n","print(high_corr)\n"],"metadata":{"id":"h1qC4yhBApWC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drop one of the correlated features (e.g., 'Quantity' and 'UnitPrice' have high correlation)\n","df = df.drop(columns=['UnitPrice'])\n"],"metadata":{"id":"1UPWxgKFd8x4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from statsmodels.stats.outliers_influence import variance_inflation_factor\n","from statsmodels.tools.tools import add_constant\n","\n","X = df[['Quantity', 'UnitPrice', 'OtherFeatures']]  # Example subset of features\n","X = add_constant(X)  # Adds constant column for intercept\n","vif_data = pd.DataFrame()\n","vif_data[\"Feature\"] = X.columns\n","vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n","print(vif_data)\n"],"metadata":{"id":"uvuvwPjEdHEF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Feature Selection"],"metadata":{"id":"2DejudWSA-a0"}},{"cell_type":"code","source":["# Select your features wisely to avoid overfitting"],"metadata":{"id":"YLhe8UmaBCEE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","# Sample dataset with correlated features\n","data = {\n","    'A': np.random.rand(100),\n","    'B': np.random.rand(100),\n","    'C': np.random.rand(100),\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# Introducing correlation between features\n","df['B'] = df['A'] * 0.9 + np.random.rand(100) * 0.1  # Making B correlated with A\n","\n","# Compute the correlation matrix\n","correlation_matrix = df.corr()\n","\n","# Set a threshold for removing highly correlated features\n","threshold = 0.9\n","\n","# Identify highly correlated features\n","highly_correlated_features = [column for column in correlation_matrix.columns if any(abs(correlation_matrix[column]) > threshold)]\n","\n","# Remove correlated features\n","df_reduced = df.drop(columns=highly_correlated_features)\n","\n","print(\"Dataset after removing highly correlated features:\\n\", df_reduced.head())\n"],"metadata":{"id":"-SdomkKUoR_Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Feature Importance using Random Forest:**"],"metadata":{"id":"Snqv3vDFsbj3"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import load_iris\n","import pandas as pd\n","\n","# Load the Iris dataset\n","data = load_iris()\n","X = pd.DataFrame(data.data, columns=data.feature_names)\n","y = data.target\n","\n","# Train a random forest model\n","rf = RandomForestClassifier()\n","rf.fit(X, y)\n","\n","# Get feature importances\n","feature_importances = rf.feature_importances_\n","\n","# Display feature importances\n","print(\"Feature Importances:\\n\", feature_importances)\n","\n","# Select features with importance above a certain threshold (e.g., 0.2)\n","selected_features = X.columns[feature_importances > 0.2]\n","X_selected = X[selected_features]\n","\n","print(\"\\nDataset after feature selection:\\n\", X_selected.head())\n"],"metadata":{"id":"S5ZTVPlyshH6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Recursive Feature Elimination (RFE):**"],"metadata":{"id":"EJCRXZQwssOg"}},{"cell_type":"code","source":["from sklearn.feature_selection import RFE\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.datasets import load_iris\n","import pandas as pd\n","\n","# Load the Iris dataset\n","data = load_iris()\n","X = pd.DataFrame(data.data, columns=data.feature_names)\n","y = data.target\n","\n","# Create a logistic regression model\n","model = LogisticRegression(max_iter=200)\n","\n","# Initialize RFE and select top 2 features\n","rfe = RFE(estimator=model, n_features_to_select=2)\n","X_rfe = rfe.fit_transform(X, y)\n","\n","# Display the selected features\n","selected_columns = X.columns[rfe.support_]\n","print(\"Selected Features: \", selected_columns)\n","\n","# Create a new DataFrame with selected features\n","X_selected = pd.DataFrame(X_rfe, columns=selected_columns)\n","print(\"\\nDataset after RFE:\\n\", X_selected.head())\n"],"metadata":{"id":"aYudAxiUswAn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Feature Selection using Lasso Regularization (L1):**"],"metadata":{"id":"qJ0K_tlfs7oD"}},{"cell_type":"code","source":["from sklearn.linear_model import Lasso\n","from sklearn.datasets import load_iris\n","import pandas as pd\n","\n","# Load the Iris dataset\n","data = load_iris()\n","X = pd.DataFrame(data.data, columns=data.feature_names)\n","y = data.target\n","\n","# Train a Lasso model (L1 regularization)\n","lasso = Lasso(alpha=0.1)\n","lasso.fit(X, y)\n","\n","# Get the feature coefficients\n","coefficients = lasso.coef_\n","\n","# Identify the features with non-zero coefficients\n","selected_features = X.columns[coefficients != 0]\n","X_selected = X[selected_features]\n","\n","print(\"Selected Features using Lasso: \", selected_features)\n","print(\"\\nDataset after Lasso feature selection:\\n\", X_selected.head())\n"],"metadata":{"id":"_4Pn_oaWtDXK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Using Cross-Validation to Evaluate Feature Selection:**"],"metadata":{"id":"jq9J7Kp9tLO9"}},{"cell_type":"code","source":["from sklearn.model_selection import cross_val_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.datasets import load_iris\n","import pandas as pd\n","\n","# Load the Iris dataset\n","data = load_iris()\n","X = pd.DataFrame(data.data, columns=data.feature_names)\n","y = data.target\n","\n","# Create a logistic regression model\n","model = LogisticRegression(max_iter=200)\n","\n","# Evaluate the model using 10-fold cross-validation\n","scores = cross_val_score(model, X, y, cv=10)\n","\n","print(\"Cross-validation scores: \", scores)\n","print(\"Average cross-validation score: \", scores.mean())\n"],"metadata":{"id":"IijCF8h1tRfH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What all feature selection methods have you used  and why?"],"metadata":{"id":"pEMng2IbBLp7"}},{"cell_type":"markdown","source":["Removing Highly Correlated Features:\n","\n","Why?: Redundant features increase model complexity and risk overfitting.\n","How?: I identified and removed highly correlated features using the correlation matrix.\n","\n","Feature Importance using Random Forest:\n","\n","Why?: Identifies which features contribute most to predictions, helping reduce unnecessary complexity.\n","How?: I used RandomForestClassifier to rank features by importance and selected the top ones.\n","\n","Recursive Feature Elimination (RFE):\n","\n","Why?: Recursively removes least important features to select the optimal set.\n","How?: I used RFE with a model (Logistic Regression) to rank and remove less important features.\n","\n","Lasso Regularization (L1):\n","\n","Why?: Lasso penalizes less relevant features by setting their coefficients to zero.\n","How?: I applied Lasso regression and selected features with non-zero coefficients."],"metadata":{"id":"rb2Lh6Z8BgGs"}},{"cell_type":"markdown","source":["##### Which all features you found important and why?"],"metadata":{"id":"rAdphbQ9Bhjc"}},{"cell_type":"markdown","source":["Quantity: Directly impacts sales and helps understand demand.\n","UnitPrice: Affects revenue and profit margins.\n","CustomerID: Enables customer segmentation and retention strategies.\n","Country: Provides insights into geographical sales patterns.\n","InvoiceDate: Helps track trends over time and forecast sales.\n","StockCode: Crucial for analyzing product-level performance.\n","These features drive business decisions related to sales, customer behavior, inventory management, and market expansion."],"metadata":{"id":"fGgaEstsBnaf"}},{"cell_type":"markdown","source":["### 5. Data Transformation"],"metadata":{"id":"TNVZ9zx19K6k"}},{"cell_type":"markdown","source":["#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"],"metadata":{"id":"nqoHp30x9hH9"}},{"cell_type":"markdown","source":["reads a dataset and checks for missing values by using the .isnull().sum() function. However, there are a couple of things to address:"],"metadata":{"id":"lpjCrOdWhcl6"}},{"cell_type":"code","source":["import pandas as pd\n","\n","  # Load your dataset\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Online Retail.xlsx - Online Retail.csv')\n","\n","  # Check for missing values\n","missing_data = df.isnull().sum()\n","print(missing_data)\n"],"metadata":{"id":"9BzHwIs7h_US"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6. Data Scaling"],"metadata":{"id":"rMDnDkt2B6du"}},{"cell_type":"code","source":["print(df.columns)\n"],"metadata":{"id":"MuVkhJ1GjHPN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Assuming you have a column 'Price' and 'Quantity', you can create 'TotalSales'\n","if 'Price' in df.columns and 'Quantity' in df.columns:\n","    df['TotalSales'] = df['Quantity'] * df['Price']\n"],"metadata":{"id":"HbE1vSoIjy1p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the columns\n","print(df.columns)\n","\n","# If you have 'Quantity' and another numeric column (like 'Price' or something similar), create 'TotalSales'\n","if 'Quantity' in df.columns and 'Price' in df.columns:\n","    df['TotalSales'] = df['Quantity'] * df['Price']\n","else:\n","    print(\"Required columns for 'TotalSales' not found\")\n"],"metadata":{"id":"TPuTHGS3kOGd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","\n","# List of columns to scale (use the actual columns you have)\n","numeric_columns = ['Quantity']  # You can add more columns if necessary, like 'TotalSales' if created\n","\n","# Initialize the StandardScaler\n","scaler = StandardScaler()\n","\n","# Apply scaling to the selected columns\n","df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n","\n","# Check the scaled data\n","print(df.head())\n"],"metadata":{"id":"v9RvyljqlE6N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which method have you used to scale you data and why?"],"metadata":{"id":"yiiVWRdJDDil"}},{"cell_type":"markdown","source":["I used StandardScaler to scale the data because it standardizes features by removing the mean and scaling them to unit variance (mean = 0, standard deviation = 1). This is ideal for algorithms that are sensitive to feature scaling, like distance-based models or gradient-based models. It ensures that no single feature dominates due to differences in scale, helping improve model performance and convergence speed."],"metadata":{"id":"BAY9lDCJlkQZ"}},{"cell_type":"markdown","source":["### 7. Dimesionality Reduction"],"metadata":{"id":"1UUpS68QDMuG"}},{"cell_type":"markdown","source":["##### Do you think that dimensionality reduction is needed? Explain Why?"],"metadata":{"id":"kexQrXU-DjzY"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"GGRlBsSGDtTQ"}},{"cell_type":"code","source":["# DImensionality Reduction (If needed)"],"metadata":{"id":"kQfvxBBHDvCa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"],"metadata":{"id":"T5CmagL3EC8N"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"ZKr75IDuEM7t"}},{"cell_type":"markdown","source":["### 8. Data Splitting"],"metadata":{"id":"BhH2vgX9EjGr"}},{"cell_type":"code","source":["# Split your data to train and test. Choose Splitting ratio wisely."],"metadata":{"id":"0CTyd2UwEyNM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What data splitting ratio have you used and why?"],"metadata":{"id":"qjKvONjwE8ra"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"Y2lJ8cobFDb_"}},{"cell_type":"markdown","source":["### 9. Handling Imbalanced Dataset"],"metadata":{"id":"P1XJ9OREExlT"}},{"cell_type":"markdown","source":["##### Do you think the dataset is imbalanced? Explain Why."],"metadata":{"id":"VFOzZv6IFROw"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"GeKDIv7pFgcC"}},{"cell_type":"code","source":["# Handling Imbalanced Dataset (If needed)"],"metadata":{"id":"nQsRhhZLFiDs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"],"metadata":{"id":"TIqpNgepFxVj"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"qbet1HwdGDTz"}},{"cell_type":"markdown","source":["## ***7. ML Model Implementation***"],"metadata":{"id":"VfCC591jGiD4"}},{"cell_type":"markdown","source":["### ML Model - 1"],"metadata":{"id":"OB4l2ZhMeS1U"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error\n"],"metadata":{"id":"q09nRuyLCwb0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QoJirW83Efuf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ML Model - 1\n","# Load the dataset\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Online Retail.xlsx - Online Retail.csv')\n","\n","# Create a 'TotalPrice' column by multiplying 'Quantity' and 'UnitPrice'\n","df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n","\n","# Select features and target\n","X = df[['Quantity', 'UnitPrice']]  # Replace with your feature columns\n","y = df['TotalPrice']  # Replace with your target column\n","\n","\n","\n","# Fit the Algorithm\n","\n","# Predict on the model"],"metadata":{"id":"7ebyywQieS1U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Split the Data into Training and Testing Sets**"],"metadata":{"id":"2ghHRD7CGmgM"}},{"cell_type":"code","source":["# Split the data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"],"metadata":{"id":"5De9r5JKGl5F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Initialize and Fit the Model**"],"metadata":{"id":"HrpvByZpGwmu"}},{"cell_type":"code","source":["# Initialize the model\n","model = LinearRegression()\n","\n","# Fit the model\n","model.fit(X_train, y_train)\n"],"metadata":{"id":"dZEogTbfG8XL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Make Predictions**"],"metadata":{"id":"33A3akfQJlU4"}},{"cell_type":"code","source":["# Make predictions\n","y_pred = model.predict(X_test)\n"],"metadata":{"id":"d-l74lxEJh80"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Evaluate the Model**"],"metadata":{"id":"RGgjIRrkJuBr"}},{"cell_type":"code","source":["# Calculate Mean Squared Error\n","mse = mean_squared_error(y_test, y_pred)\n","print(f'Mean Squared Error: {mse}')\n"],"metadata":{"id":"7mUIRgAkJ0mD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"_2T9gYbqG5IP"}},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"ArJBuiUVfxKd"}},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","import numpy as np\n","\n","# Calculate Mean Squared Error\n","mse = mean_squared_error(y_test, y_pred)\n","print(f'Mean Squared Error: {mse}')\n","\n","# Calculate Mean Absolute Error\n","mae = mean_absolute_error(y_test, y_pred)\n","print(f'Mean Absolute Error: {mae}')\n","\n","# Calculate R-squared\n","r2 = r2_score(y_test, y_pred)\n","print(f'R-squared: {r2}')\n","\n","# Calculate Root Mean Squared Error\n","rmse = np.sqrt(mse)\n","print(f'Root Mean Squared Error: {rmse}')\n","\n"],"metadata":{"id":"rqD5ZohzfxKe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Visualizing Performance with a Score Chart:**"],"metadata":{"id":"x5e8YRz-MTak"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Metrics\n","metrics = ['MSE', 'MAE', 'R²', 'RMSE']\n","values = [mse, mae, r2, rmse]\n","\n","# Create bar chart\n","plt.bar(metrics, values, color=['blue', 'green', 'red', 'purple'])\n","plt.title('Model Performance Metrics')\n","plt.ylabel('Score')"],"metadata":{"id":"fRF9KTyjMLhE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"4qY1EAkEfxKe"}},{"cell_type":"markdown","source":["**Implementing Cross-Validation:**"],"metadata":{"id":"MvaL8BgYNfoL"}},{"cell_type":"code","source":["from sklearn.model_selection import cross_val_score\n","from sklearn.linear_model import LinearRegression\n","\n","# Initialize the model\n","model = LinearRegression()\n","\n","# Perform 5-fold cross-validation\n","cv_scores = cross_val_score(model, X, y, cv=5)\n","\n","# Print the cross-validation scores\n","print(f'Cross-Validation Scores: {cv_scores}')\n","print(f'Mean CV Score: {cv_scores.mean()}')\n"],"metadata":{"id":"g60IZT7XNi9F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" **Hyperparameter Tuning**"],"metadata":{"id":"to1Io25YNn5v"}},{"cell_type":"markdown","source":["**GridSearchCV**"],"metadata":{"id":"yO8fc6rNNh1G"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","from sklearn.svm import SVC\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","\n","# Load dataset\n","data = load_iris()\n","X = data.data\n","y = data.target\n","\n","# Split data into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Define the model\n","model = SVC()\n","\n","# Define the parameter grid\n","param_grid = {\n","    'C': [0.1, 1, 10],\n","    'kernel': ['linear', 'rbf'],\n","    'gamma': ['scale', 'auto']\n","}\n","\n","# Initialize GridSearchCV\n","grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n","\n","# Fit the model\n","grid_search.fit(X_train, y_train)\n","\n","# Print the best parameters and score\n","print(f'Best Parameters: {grid_search.best_params_}')\n","print(f'Best Score: {grid_search.best_score_}')\n"],"metadata":{"id":"x4n5Qk4IVQLG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**RandomizedSearchCV**"],"metadata":{"id":"WcZO3eARYsXx"}},{"cell_type":"code","source":["from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.svm import SVC\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from scipy.stats import uniform\n","\n","# Load dataset\n","data = load_iris()\n","X = data.data\n","y = data.target\n","\n","# Split data into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Define the model\n","model = SVC()\n","\n","# Define the parameter distributions\n","param_dist = {\n","    'C': uniform(0.1, 10),  # Uniform distribution between 0.1 and 10\n","    'kernel': ['linear', 'rbf', 'poly'],\n","    'gamma': ['scale', 'auto'] + list(np.logspace(-3, 3, 50))\n","}\n","\n","# Initialize RandomizedSearchCV\n","randomized_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=20, cv=5, scoring='accuracy', random_state=42)\n","\n","# Fit the model\n","randomized_search.fit(X_train, y_train)\n","\n","# Print the best parameters and score\n","print(f'Best Parameters: {randomized_search.best_params_}')\n","print(f'Best Score: {randomized_search.best_score_}')\n"],"metadata":{"id":"0B688haSXYn0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**BayesianOptimization**"],"metadata":{"id":"5I1-GnPvbRzS"}},{"cell_type":"code","source":["pip install bayesian-optimization\n"],"metadata":{"id":"mXZqMzA7aC4w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%pip install bayesian-optimization\n"],"metadata":{"id":"_llp3FTMak2q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from bayes_opt import BayesianOptimization\n","from sklearn.svm import SVC\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset\n","data = load_iris()\n","X = data.data\n","y = data.target\n","\n","# Split data into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Define the function to optimize\n","def svm_evaluate(C, gamma):\n","    model = SVC(C=C, gamma=gamma)\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_test)\n","    return accuracy_score(y_test, y_pred)\n","\n","# Define the parameter bounds\n","pbounds = {'C': (0.1, 10), 'gamma': (0.001, 1)}\n","\n","# Initialize Bayesian Optimization\n","optimizer = BayesianOptimization(\n","    f=svm_evaluate,\n","    pbounds=pbounds,\n","    random_state=42,\n",")\n","\n","# Perform optimization\n","optimizer.maximize(init_points=5, n_iter=25)\n","\n","# Print the best parameters and score\n","print(f'Best Parameters: {optimizer.max[\"params\"]}')\n","print(f'Best Score: {optimizer.max[\"target\"]}')\n"],"metadata":{"id":"UrRi733wNfRf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n","\n","# Fit the Algorithm\n","\n","# Predict on the model"],"metadata":{"id":"Dy61ujd6fxKe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.columns"],"metadata":{"id":"ECSa6VRCekEJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"PiV4Ypx8fxKe"}},{"cell_type":"markdown","source":["GridSearchCV: Tests all combinations of hyperparameters. Use when the search space is small and you need exhaustive testing, but it can be computationally expensive.\n","\n","RandomizedSearchCV: Samples random combinations of hyperparameters. Use when the search space is large, and you want a faster, less computationally intensive option.\n","\n","Bayesian Optimization: Uses past results to intelligently select the next set of hyperparameters to test. Use when you want to optimize efficiently with fewer evaluations, especially in large or complex search spaces."],"metadata":{"id":"negyGRa7fxKf"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"TfvqoZmBfxKf"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"OaLui8CcfxKf"}},{"cell_type":"markdown","source":["### ML Model - 2"],"metadata":{"id":"dJ2tPlVmpsJ0"}},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"JWYfwnehpsJ1"}},{"cell_type":"code","source":["/content/drive/MyDrive/Colab Notebooks/Online Retail.xlsx - Online Retail.csv"],"metadata":{"id":"39i8V30mh4bN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split the data into train and test sets\n","# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# # Standardize the features (important for SVR)\n","# scaler_X = StandardScaler()\n","# scaler_y = StandardScaler()\n","\n","# X_train_scaled = scaler_X.fit_transform(X_train)\n","# X_test_scaled = scaler_X.transform(X_test)\n","# y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n","# y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1))\n","\n","# # Initialize the Support Vector Regression model\n","# svr_model = SVR(kernel='rbf')  # Radial basis function kernel\n","\n","# # Train the model\n","# svr_model.fit(X_train_scaled, y_train_scaled.ravel())\n","\n","# # Make predictions\n","# y_pred_svr_scaled = svr_model.predict(X_test_scaled)\n","# y_pred_svr = scaler_y.inverse_transform(y_pred_svr_scaled.reshape(-1, 1))\n","\n","# # Evaluate the model\n","# mse_svr = mean_squared_error(y_test, y_pred_svr)\n","# mae_svr = mean_absolute_error(y_test, y_pred_svr)\n","# r2_svr = r2_score(y_test, y_pred_svr)\n","# rmse_svr = np.sqrt(mse_svr)\n","\n","# # Print the evaluation metrics\n","# print(f'SVR - MSE: {mse_svr}')\n","# print(f'SVR - MAE: {mae_svr}')\n","# print(f'SVR - R²: {r2_svr}')\n","# print(f'SVR - RMSE: {rmse_svr}')\n"],"metadata":{"id":"v2Jy8r0uiqwk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df\n"],"metadata":{"id":"i_kroJ9oiKGn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n"],"metadata":{"id":"yEl-hgQWpsJ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVR\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","from sklearn.preprocessing import StandardScaler\n","\n","# Load the dataset\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Online Retail.xlsx - Online Retail.csv')\n","\n","# Create a 'TotalPrice' column by multiplying 'Quantity' and 'UnitPrice'\n","df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n","\n","# Drop rows with missing or zero values for Quantity, UnitPrice, and TotalPrice\n","df = df.dropna(subset=['Quantity', 'UnitPrice'])\n","df = df[df['Quantity'] > 0]\n","df = df[df['UnitPrice'] > 0]\n","\n","# Select features and target\n","X = df[['UnitPrice', 'TotalPrice']]  # Features: 'UnitPrice' and 'TotalPrice'\n","y = df['Quantity']  # Target: Predict 'Quantity'\n","\n","# Split the data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize the features (important for SVR)\n","scaler_X = StandardScaler()\n","scaler_y = StandardScaler()\n","\n","X_train_scaled = scaler_X.fit_transform(X_train)\n","X_test_scaled = scaler_X.transform(X_test)\n","y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n","y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1))\n","\n","# Initialize the Support Vector Regression model\n","svr_model = SVR(kernel='rbf')  # Radial basis function kernel\n","\n","# Train the model\n","svr_model.fit(X_train_scaled, y_train_scaled.ravel())\n","\n","# Make predictions\n","y_pred_svr_scaled = svr_model.predict(X_test_scaled)\n","y_pred_svr = scaler_y.inverse_transform(y_pred_svr_scaled.reshape(-1, 1))\n","\n","# Evaluate the model\n","mse_svr = mean_squared_error(y_test, y_pred_svr)\n","mae_svr = mean_absolute_error(y_test, y_pred_svr)\n","r2_svr = r2_score(y_test, y_pred_svr)\n","rmse_svr = np.sqrt(mse_svr)\n","\n","# Metrics for visualization\n","metrics = ['MSE', 'MAE', 'R²', 'RMSE']\n","values = [mse_svr, mae_svr, r2_svr, rmse_svr]\n","\n","# Plot\n"],"metadata":{"id":"s4Fqu0UAlmdb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"-jK_YjpMpsJ2"}},{"cell_type":"code","source":["# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n","\n","# Fit the Algorithm\n","\n","# Predict on the model"],"metadata":{"id":"Dn0EOfS6psJ2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"HAih1iBOpsJ2"}},{"cell_type":"markdown","source":["GridSearchCV: Ideal when the hyperparameter space is small, and computational resources are sufficient.\n","\n","RandomizedSearchCV: Suitable for larger hyperparameter spaces where a balance between exploration and computational efficiency is needed.\n","\n","Bayesian Optimization: Best for scenarios where each evaluation is expensive, and the goal is to find the optimal hyperparameters with minimal evaluations."],"metadata":{"id":"9kBgjYcdpsJ2"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"zVGeBEFhpsJ2"}},{"cell_type":"markdown","source":["Accuracy Score: Measures the proportion of correct predictions.\n","\n","Precision and Recall: Evaluate the model's performance in terms of false positives and false negatives.\n","\n","F1-Score: The harmonic mean of precision and recall, providing a balance between the two.\n","\n","ROC-AUC Curve: Illustrates the trade-off between true positive rate and false positive rate."],"metadata":{"id":"74yRdG6UpsJ3"}},{"cell_type":"markdown","source":["#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."],"metadata":{"id":"bmKjuQ-FpsJ3"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"BDKtOrBQpsJ3"}},{"cell_type":"markdown","source":["### ML Model - 3"],"metadata":{"id":"Fze-IPXLpx6K"}},{"cell_type":"code","source":["# ML Model - 3 Implementationdf = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Online Retail.xlsx - Online Retail.csv')\n","\n","# Create a 'TotalPrice' column by multiplying 'Quantity' and 'UnitPrice'\n","df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n","\n","# Select features and target\n","X = df[['Quantity', 'UnitPrice']]  # Replace with your feature columns\n","y = df['TotalPrice']\n","\n","# Fit the Algorithm\n","\n","# Predict on the model"],"metadata":{"id":"FFrSXAtrpx6M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"7AN1z2sKpx6M"}},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","import numpy as np\n","\n","# Calculate Mean Squared Error\n","mse = mean_squared_error(y_test, y_pred)\n","print(f'Mean Squared Error: {mse}')\n","\n","# Calculate Mean Absolute Error\n","mae = mean_absolute_error(y_test, y_pred)\n","print(f'Mean Absolute Error: {mae}')\n","\n","# Calculate R-squared\n","r2 = r2_score(y_test, y_pred)\n","print(f'R-squared: {r2}')\n","\n","# Calculate Root Mean Squared Error\n","rmse = np.sqrt(mse)\n","print(f'Root Mean Squared Error: {rmse}')\n","\n"],"metadata":{"id":"xIY4lxxGpx6M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"9PIHJqyupx6M"}},{"cell_type":"markdown","source":["**Cross- Validation**"],"metadata":{"id":"0EYQ31rMWkyw"}},{"cell_type":"code","source":["from sklearn.model_selection import cross_val_score\n","from sklearn.linear_model import LinearRegression\n","\n","# Initialize the model\n","model = LinearRegression()\n","\n","# Perform 5-fold cross-validation\n","cv_scores = cross_val_score(model, X, y, cv=5)\n","\n","# Print the cross-validation scores\n","print(f'Cross-Validation Scores: {cv_scores}')\n","print(f'Mean CV Score: {cv_scores.mean()}')\n"],"metadata":{"id":"TF1yA5lrWuWa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**GridSearchCV**"],"metadata":{"id":"F-6hZ5l2W_70"}},{"cell_type":"code","source":["rom sklearn.model_selection import GridSearchCV\n","from sklearn.svm import SVC\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","\n","# Load dataset\n","data = load_iris()\n","X = data.data\n","y = data.target\n","\n","# Split data into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Define the model\n","model = SVC()\n","\n","# Define the parameter grid\n","param_grid = {\n","    'C': [0.1, 1, 10],\n","    'kernel': ['linear', 'rbf'],\n","    'gamma': ['scale', 'auto']\n","}\n","\n","# Initialize GridSearchCV\n","grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n","\n","# Fit the model\n","grid_search.fit(X_train, y_train)\n","\n","# Print the best parameters and score\n","print(f'Best Parameters: {grid_search.best_params_}')\n","print(f'Best Score: {grid_search.best_score_}')\n"],"metadata":{"id":"B7uJ1lRXW9_6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**RandomSearchCV**"],"metadata":{"id":"fbKeHu_kXNpk"}},{"cell_type":"code","source":["from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.svm import SVC\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from scipy.stats import uniform\n","\n","# Load dataset\n","data = load_iris()\n","X = data.data\n","y = data.target\n","\n","# Split data into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Define the model\n","model = SVC()\n","\n","# Define the parameter distributions\n","param_dist = {\n","    'C': uniform(0.1, 10),  # Uniform distribution between 0.1 and 10\n","    'kernel': ['linear', 'rbf', 'poly'],\n","    'gamma': ['scale', 'auto'] + list(np.logspace(-3, 3, 50))\n","}\n","\n","# Initialize RandomizedSearchCV\n","randomized_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=20, cv=5, scoring='accuracy', random_state=42)\n","\n","# Fit the model\n","randomized_search.fit(X_train, y_train)\n","\n","# Print the best parameters and score\n","print(f'Best Parameters: {randomized_search.best_params_}')\n","print(f'Best Score: {randomized_search.best_score_}')"],"metadata":{"id":"3SyEVAfkXMMb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**BayesianOptimization**"],"metadata":{"id":"IGb2qCsoXjSJ"}},{"cell_type":"code","source":["from bayes_opt import BayesianOptimization\n","from sklearn.svm import SVC\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset\n","data = load_iris()\n","X = data.data\n","y = data.target\n","\n","# Split data into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Define the function to optimize\n","def svm_evaluate(C, gamma):\n","    model = SVC(C=C, gamma=gamma)\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_test)\n","    return accuracy_score(y_test, y_pred)\n","\n","# Define the parameter bounds\n","pbounds = {'C': (0.1, 10), 'gamma': (0.001, 1)}\n","\n","# Initialize Bayesian Optimization\n","optimizer = BayesianOptimization(\n","    f=svm_evaluate,\n","    pbounds=pbounds,\n","    random_state=42,\n",")\n","\n","# Perform optimization\n","optimizer.maximize(init_points=5, n_iter=25)"],"metadata":{"id":"8vBsLAMQXgqS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"_-qAgymDpx6N"}},{"cell_type":"markdown","source":["GridSearchCV: Tests all combinations of hyperparameters. Use when the search space is small and you need exhaustive testing, but it can be computationally expensive.\n","\n","RandomizedSearchCV: Samples random combinations of hyperparameters. Use when the search space is large, and you want a faster, less computationally intensive option.\n","\n","Bayesian Optimization: Uses past results to intelligently select the next set of hyperparameters to test. Use when you want to optimize efficiently with fewer evaluations, especially in large or complex search spaces."],"metadata":{"id":"lQMffxkwpx6N"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"Z-hykwinpx6N"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"MzVzZC6opx6N"}},{"cell_type":"markdown","source":["### 1. Which Evaluation metrics did you consider for a positive business impact and why?"],"metadata":{"id":"h_CCil-SKHpo"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"jHVz9hHDKFms"}},{"cell_type":"markdown","source":["### 2. Which ML model did you choose from the above created models as your final prediction model and why?"],"metadata":{"id":"cBFFvTBNJzUa"}},{"cell_type":"markdown","source":["ML-1 because it is easy for  anlysis and get a clear and understandable result."],"metadata":{"id":"6ksF5Q1LKTVm"}},{"cell_type":"markdown","source":["### 3. Explain the model which you have used and the feature importance using any model explainability tool?"],"metadata":{"id":"HvGl1hHyA_VK"}},{"cell_type":"markdown","source":["In this case, I used Linear Regression as a model to predict the TotalPrice based on the features Quantity and UnitPrice. Additionally, I will explain how to analyze feature importance using a model explainability tool, particularly for more complex models like Random Forest Regressor, as linear regression is straightforward and its feature importance can be directly derived from the coefficients."],"metadata":{"id":"YnvVTiIxBL-C"}},{"cell_type":"markdown","source":["## ***8.*** ***Future Work (Optional)***"],"metadata":{"id":"EyNgTHvd2WFk"}},{"cell_type":"markdown","source":["### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"],"metadata":{"id":"KH5McJBi2d8v"}},{"cell_type":"code","source":["# Save the File"],"metadata":{"id":"bQIANRl32f4J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"],"metadata":{"id":"iW_Lq9qf2h6X"}},{"cell_type":"code","source":["# Load the File and predict unseen data."],"metadata":{"id":"oEXk9ydD2nVC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"],"metadata":{"id":"-Kee-DAl2viO"}},{"cell_type":"markdown","source":["# **Conclusion**"],"metadata":{"id":"gCX9965dhzqZ"}},{"cell_type":"markdown","source":["In summary, we used Linear Regression and Random Forest Regressor to predict TotalPrice based on Quantity and UnitPrice. Linear Regression provided simple feature importance based on coefficients, while Random Forest gave more robust insights using feature importance derived from decision trees.\n","\n","For model explainability, we discussed tools like SHAP and LIME, which help understand how individual features influence predictions, especially for more complex models like Random Forest.\n","\n","From a business perspective, Quantity plays a more significant role in predicting TotalPrice than UnitPrice, which can inform pricing and sales strategies. The combination of accurate models and explainability tools allows for data-driven decision-making and enhances business insights."],"metadata":{"id":"Fjb1IsQkh3yE"}},{"cell_type":"markdown","source":["### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"],"metadata":{"id":"gIfDvo9L0UH2"}}]}